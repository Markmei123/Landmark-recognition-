{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMVO5GIybAbBWXI6NNPBgDH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Markmei123/Landmark-recognition-/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctdvY6PbJ5-R",
        "outputId": "ba7ac7ab-6111-432c-d54e-aeabcbcebcd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "qgRSnJ_AGLF8",
        "outputId": "86a3849f-91a4-4dca-ecca-42a787d1d38f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "google-landmark  sample_data  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdOe0XsYA6jR",
        "outputId": "fd1300d1-5784-4381-cb4c-5a278428ed65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'google-landmark'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 109 (delta 6), reused 10 (delta 6), pack-reused 93 (from 1)\u001b[K\n",
            "Receiving objects: 100% (109/109), 30.66 KiB | 3.41 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cvdfoundation/google-landmark.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd google-landmark/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N0kd5--BFsJ",
        "outputId": "2514008e-f3b7-4c01-8dae-090dd81ada7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/google-landmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train"
      ],
      "metadata": {
        "id": "n7snn48nC-Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BjsdIFeDVCB",
        "outputId": "22afbc02-c5ba-4add-d1ed-ad55e1e91129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/google-landmark/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ../download-dataset.sh train 5"
      ],
      "metadata": {
        "id": "fTGy73jGDXV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389e9e6e-7f9f-4298-da71-c8088d6f3764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading images_000.tar and its md5sum...\n",
            "Downloading images_001.tar and its md5sum...\n",
            "Downloading images_002.tar and its md5sum...\n",
            "Downloading images_003.tar and its md5sum...\n",
            "Downloading images_004.tar and its md5sum...\n",
            "Downloading images_005.tar and its md5sum...\n",
            "images_005.tar extracted!\n",
            "images_001.tar extracted!\n",
            "images_000.tar extracted!\n",
            "images_002.tar extracted!\n",
            "images_003.tar extracted!\n",
            "images_004.tar extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJC3KKmOcQDj",
        "outputId": "a43adfb0-2c49-419d-f87b-c5c042c82e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m41.0/43.2 kB\u001b[0m \u001b[31m982.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m741.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define ViT model"
      ],
      "metadata": {
        "id": "0nTxZNa1Hve2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helpers\n",
        "\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "# classes\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
        "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        return self.norm(x)\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "B7tjfk2MHu_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing and training"
      ],
      "metadata": {
        "id": "JZOUVDACH6yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "# from Visiontransformer import ViT\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        # reading csv file and intialization\n",
        "        self.labels_df = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # filter exist labels\n",
        "        self.labels_df = self.labels_df[self.labels_df.apply(self._check_image_exists, axis=1)].reset_index(drop=True)\n",
        "\n",
        "        # get unique label and create mapping\n",
        "        self.label_to_index = {label: idx for idx, label in enumerate(self.labels_df['landmark_id'].unique())}\n",
        "\n",
        "    def _check_image_exists(self, row):\n",
        "        img_id = row[0]\n",
        "        img_path = os.path.join(self.root_dir, img_id[0], img_id[1], img_id[2], f\"{img_id}.jpg\")\n",
        "        return os.path.exists(img_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.labels_df.iloc[idx, 0]\n",
        "        original_label = self.labels_df.iloc[idx, 2]\n",
        "\n",
        "        # mapping labels in sequence\n",
        "        label = self.label_to_index[original_label]\n",
        "\n",
        "        # construct path of img\n",
        "        img_path = os.path.join(self.root_dir, img_id[0], img_id[1], img_id[2], f\"{img_id}.jpg\")\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "print(\"--------------start-----------------\")\n",
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# adjust the path of file above before running\n",
        "train_dataset = CustomImageDataset(csv_file='../train.csv', root_dir='./train', transform=transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
        "\n",
        "used_labels = train_dataset.labels_df['landmark_id']\n",
        "\n",
        "# get all unique label\n",
        "unique_used_labels = used_labels.unique()\n",
        "\n",
        "# get the number of labels\n",
        "num_unique_used_labels = len(unique_used_labels)\n",
        "\n",
        "print(f\"there are  {num_unique_used_labels} different labels in the dataset\")\n",
        "\n",
        "model = ViT(\n",
        "    image_size=224,\n",
        "    patch_size=16,\n",
        "    num_classes=num_unique_used_labels,\n",
        "    dim=128,\n",
        "    depth=8,\n",
        "    heads=6,\n",
        "    mlp_dim=236,\n",
        "    dropout=0.1,\n",
        "    emb_dropout=0.1\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, epochs=20):\n",
        "    for imgs, labels in train_loader:\n",
        "        print(imgs.shape, labels.shape)\n",
        "        break\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"epoch:{epoch} in {epochs}\")\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device).long()\n",
        "\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader)}, Accuracy: {correct / len(train_loader.dataset)}\")\n",
        "\n",
        "train(model, train_loader, criterion, optimizer)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WZmL5NzgH1bF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f2dcd5f-576f-4b7b-bf00-c2561ee40afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------start-----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-4d3008baa886>:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  img_id = row[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are  6366 different labels in the dataset\n",
            "torch.Size([32, 3, 224, 224]) torch.Size([32])\n",
            "epoch:0 in 20\n",
            "Epoch 1/20, Loss: 8.769519344452888, Accuracy: 0.0\n",
            "epoch:1 in 20\n",
            "Epoch 2/20, Loss: 8.901951662406393, Accuracy: 0.0\n",
            "epoch:2 in 20\n",
            "Epoch 3/20, Loss: 8.909409914148569, Accuracy: 0.0007224389539083947\n",
            "epoch:3 in 20\n",
            "Epoch 4/20, Loss: 8.793317684929491, Accuracy: 0.0015893656985984685\n",
            "epoch:4 in 20\n",
            "Epoch 5/20, Loss: 8.626118796212333, Accuracy: 0.002167316861725184\n",
            "epoch:5 in 20\n",
            "Epoch 6/20, Loss: 8.436985138923891, Accuracy: 0.0014448779078167894\n",
            "epoch:6 in 20\n",
            "Epoch 7/20, Loss: 8.19995519532586, Accuracy: 0.0017338534893801473\n",
            "epoch:7 in 20\n",
            "Epoch 8/20, Loss: 7.892975130388813, Accuracy: 0.0020228290709435053\n",
            "epoch:8 in 20\n",
            "Epoch 9/20, Loss: 7.590631430050195, Accuracy: 0.0024562924432885423\n",
            "epoch:9 in 20\n",
            "Epoch 10/20, Loss: 7.328610316948956, Accuracy: 0.0015893656985984685\n",
            "epoch:10 in 20\n",
            "Epoch 11/20, Loss: 7.09875695057179, Accuracy: 0.004334633723450368\n",
            "epoch:11 in 20\n",
            "Epoch 12/20, Loss: 6.819836620910926, Accuracy: 0.005057072677358763\n",
            "epoch:12 in 20\n",
            "Epoch 13/20, Loss: 6.487769153260965, Accuracy: 0.005635023840485479\n",
            "epoch:13 in 20\n",
            "Epoch 14/20, Loss: 6.060766529927056, Accuracy: 0.011414535471752637\n",
            "epoch:14 in 20\n",
            "Epoch 15/20, Loss: 5.549021072651384, Accuracy: 0.02326253431585031\n",
            "epoch:15 in 20\n",
            "Epoch 16/20, Loss: 4.92654703509423, Accuracy: 0.06631989596879063\n",
            "epoch:16 in 20\n",
            "Epoch 17/20, Loss: 4.246960198274955, Accuracy: 0.14564369310793238\n",
            "epoch:17 in 20\n",
            "Epoch 18/20, Loss: 3.472005886965633, Accuracy: 0.29128738621586475\n",
            "epoch:18 in 20\n",
            "Epoch 19/20, Loss: 2.6632195623239614, Accuracy: 0.4723305880653085\n",
            "epoch:19 in 20\n",
            "Epoch 20/20, Loss: 1.8978368900887976, Accuracy: 0.6455714492125415\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Markmei123/Landmark-recognition-/blob/main/CNN%2BVIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuHDTCMfKdtm",
        "outputId": "ea4e2f9f-3fe0-4816-972f-7c4cde74a5c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  501M  100  501M    0     0  13.9M      0  0:00:36  0:00:36 --:--:-- 15.4M\n",
            "curl: (3) URL using bad/illegal format or missing URL\n"
          ]
        }
      ],
      "source": [
        "!curl -o train.csv https://s3.amazonaws.com/google-landmark/metadata/train.csv /content/landmark-recognition-2021"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctdvY6PbJ5-R",
        "outputId": "ba7ac7ab-6111-432c-d54e-aeabcbcebcd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgRSnJ_AGLF8",
        "outputId": "86a3849f-91a4-4dca-ecca-42a787d1d38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "google-landmark  sample_data  train.csv\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdOe0XsYA6jR",
        "outputId": "1499db8c-2f22-44b3-d78d-fa87a93c37b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'google-landmark'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 109 (delta 6), reused 10 (delta 6), pack-reused 93 (from 1)\u001b[K\n",
            "Receiving objects: 100% (109/109), 30.66 KiB | 10.22 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cvdfoundation/google-landmark.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N0kd5--BFsJ",
        "outputId": "e584830e-0cae-472c-fe85-95b7ea58fd52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/google-landmark\n"
          ]
        }
      ],
      "source": [
        "%cd google-landmark/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7snn48nC-Ua"
      },
      "outputs": [],
      "source": [
        "!mkdir train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BjsdIFeDVCB",
        "outputId": "57a5e40f-d735-4603-d987-7a001028d994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/google-landmark/train\n"
          ]
        }
      ],
      "source": [
        "%cd train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTGy73jGDXV0",
        "outputId": "89069444-e505-4519-b0ce-cebc29419ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading images_000.tar and its md5sum...\n",
            "Downloading images_001.tar and its md5sum...\n",
            "Downloading images_002.tar and its md5sum...\n",
            "Downloading images_003.tar and its md5sum...\n",
            "Downloading images_004.tar and its md5sum...\n",
            "Downloading images_005.tar and its md5sum...\n",
            "images_004.tar extracted!\n",
            "images_000.tar extracted!\n",
            "images_003.tar extracted!\n",
            "images_002.tar extracted!\n",
            "images_005.tar extracted!\n",
            "images_001.tar extracted!\n"
          ]
        }
      ],
      "source": [
        "!bash ../download-dataset.sh train 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJC3KKmOcQDj",
        "outputId": "d0dfe127-0de3-490e-95a0-d97a2c6e789f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nTxZNa1Hve2"
      },
      "source": [
        "Define ViT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7tjfk2MHu_H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# Helper function\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "# Pre-trained CNN for Feature Extraction (ResNet50)\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNFeatureExtractor, self).__init__()\n",
        "        self.cnn = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-2])  # Remove fully connected layers\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling to reduce dimensionality\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.cnn(x)  # Extract CNN features\n",
        "            features = self.global_avg_pool(features)  # Apply Global Average Pooling\n",
        "            features = torch.flatten(features, 1)  # Flatten to (batch_size, 2048)\n",
        "        return features\n",
        "\n",
        "# ViT Model\n",
        "class ViTWithCNN(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool='cls', channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        # ViT components\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        # Pretrained CNN (ResNet) for feature extraction\n",
        "        self.cnn_feature_extractor = CNNFeatureExtractor()\n",
        "\n",
        "        # Calculate feature dimensions after CNN and ViT\n",
        "        self.cnn_output_dim = 2048  # After Global Average Pooling, CNN output is 2048\n",
        "        self.vit_output_dim = dim  # ViT embedding dimension\n",
        "\n",
        "        # Concatenate CNN and ViT features\n",
        "        self.concat_mlp = nn.Linear(self.cnn_output_dim + self.vit_output_dim, dim)\n",
        "\n",
        "        # Final classification head\n",
        "        self.mlp_head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # ViT Embedding\n",
        "        vit_emb = self.to_patch_embedding(img)\n",
        "        b, n, _ = vit_emb.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=b)\n",
        "        vit_emb = torch.cat((cls_tokens, vit_emb), dim=1)\n",
        "        vit_emb += self.pos_embedding[:, :(n + 1)]\n",
        "        vit_emb = self.dropout(vit_emb)\n",
        "        vit_emb = self.transformer(vit_emb)\n",
        "\n",
        "        vit_emb = vit_emb.mean(dim=1) if self.pool == 'mean' else vit_emb[:, 0]\n",
        "\n",
        "        # CNN Feature Extraction with Global Average Pooling\n",
        "        cnn_features = self.cnn_feature_extractor(img)\n",
        "\n",
        "        # Concatenate CNN and ViT features\n",
        "        combined_features = torch.cat([vit_emb, cnn_features], dim=1)\n",
        "        combined_features = self.concat_mlp(combined_features)\n",
        "\n",
        "        # Final classification\n",
        "        return self.mlp_head(combined_features)\n",
        "\n",
        "# Custom Dataset Class\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.labels_df = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.labels_df = self.labels_df[self.labels_df.apply(self._check_image_exists, axis=1)].reset_index(drop=True)\n",
        "        self.label_to_index = {label: idx for idx, label in enumerate(self.labels_df['landmark_id'].unique())}\n",
        "\n",
        "    def _check_image_exists(self, row):\n",
        "        img_id = row.iloc[0]\n",
        "        img_path = os.path.join(self.root_dir, img_id[0], img_id[1], img_id[2], f\"{img_id}.jpg\")\n",
        "        return os.path.exists(img_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.labels_df.iloc[idx, 0]\n",
        "        original_label = self.labels_df.iloc[idx, 2]\n",
        "        label = self.label_to_index[original_label]\n",
        "\n",
        "        img_path = os.path.join(self.root_dir, img_id[0], img_id[1], img_id[2], f\"{img_id}.jpg\")\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Data Augmentation and Preprocessing\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),  # Reduced resolution from 224x224 to 160x160 for faster training # CHANGED\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset and DataLoader\n",
        "train_dataset = CustomImageDataset(csv_file='../train.csv', root_dir='./train', transform=train_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)  # Increased batch size to 64 for faster training # CHANGED\n",
        "\n",
        "# Get number of unique labels\n",
        "used_labels = train_dataset.labels_df['landmark_id']\n",
        "unique_used_labels = used_labels.unique()\n",
        "num_unique_used_labels = len(unique_used_labels)\n",
        "\n",
        "# Initialize model\n",
        "model = ViTWithCNN(\n",
        "    image_size=160,  # Adjusted for smaller input image size # CHANGED\n",
        "    patch_size=16,\n",
        "    num_classes=num_unique_used_labels,\n",
        "    dim=192,  # Balanced dimension\n",
        "    depth=8,  # Transformer depth\n",
        "    heads=8,  # Transformer heads\n",
        "    mlp_dim=384,\n",
        "    dropout=0.1,\n",
        "    emb_dropout=0.1\n",
        ")\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# AMP for mixed precision\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "# Set up device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training loop with AMP (mixed precision)\n",
        "def train(model, train_loader, criterion, optimizer, scheduler, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Running Epoch {epoch+1}/{epochs}\")\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device).long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Use mixed precision with autocast (updated with 'cuda')\n",
        "            with autocast(device_type='cuda'):\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # Scale loss and backpropagate\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step(total_loss / len(train_loader))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader)}, Accuracy: {correct / len(train_loader.dataset)}\")\n",
        "\n",
        "# Start training\n",
        "train(model, train_loader, criterion, optimizer, scheduler)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}